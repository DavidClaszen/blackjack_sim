---
title: "ISYE 6644 Simulation: To Hit or Not to Hit, That is the Question"
author: "Group 162: David Claszen"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "Output") })
---

```{r setup, include=FALSE}
# I have included an estimated run time for taxing code chunks. 
# These are from my relatively rubbish laptop, so
# as long as you're not using a potato, this should run faster
# Knitting to PDF is also faster than running the chunks, 
# I have timed it to around 7 minutes total

knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(janitor)
library(kableExtra)
library(patchwork)
source("play_blackjack.R")
```


# Abstract

Blackjack is a well-known casino card game that has captured the attention of many players hoping to beat the house through various strategies. A brief overview of the game is given, followed by a literature review and explanation of multiple strategies. Code was developed in R to simulate independent blackjack games, which is explained, tested, and validated. Using independent replications, I explore and compare the profitability of 4 strategies for independent games: Baldwin *et al.*’s basic strategy, an improvement on the basic strategy by Thorp, a simplified version of this basic strategy, and mimicking the dealer. 

# Background and Description of Problem

## Blackjack

Blackjack is a gambling casino card game with around 2 to 7 players using one or multiple 52 card decks. Each round, the players place an initial bet and are then dealt 2 cards each by the dealer. The dealer deals him/herself two cards as well, the first of which visible, face-up, and the second face down (the hole card). The goal is to win money by creating hands with a value higher than the dealer's hand but not exceeding 21, which would count as a loss or bust. Each player gets to decide whether to "hit" and get another card, to "double" their bet and get one final card and finish, to "split" their current hand into two new hands (only for cards with equal value and requiring another bet as well), or to "stand" and finish their turn with their current hand. After the players have finished their hands, the dealer draws until their hand achieves a total of 17 or higher, but never doubles or splits. If the dealer busts, all players that didn't bust win an amount equal to their bet, but players who already went bust lose even if the dealer busts. If the player and dealer tie (a "push"), the bet is returned. Other possible moves which aren't considered below are the ability to surrender (losing only half your bet), or to buy insurance when the dealer's shown card is an ace.

All cards with a numerical value (2 to 10) are worth as many points as their number. All face cards (kings, queens, etc.) are worth 10 points. An ace can be worth either 1 or 11 points depending on whether counting it as 11 would push the total beyond 21 or not. A natural happens when an initial hand consists of an ace and a face card, totaling 21. If only the player holds a natural, he receives 1.5 times his original bet. If both the player and dealer hold a natural, it is another push and the bet is returned. One mayor rule variation is whether the dealer has to hit or stand on a soft 17. A soft hand is a hand with an ace that can be counted as 11 without the total exceeding 21; that is, the ace can still be counted as a 1 if further draws push the total beyond 21. The dealer having to stand on soft 17 is commonly referred to as S17 and having to hit as H17. 

## Blackjack Strategies

The main problem is the simulation and analysis of multiple blackjack strategies and to determine which choice maximizes profit. The player decision whether to stand or hit, given a hand of cards and a visible dealer card, had already been solved mathematically since Baldwin *et al.*'s 1956 paper, "The Optimum Strategy in Blackjack".^[Roger R. Baldwin, Wilbert E. Cantey, Herbert Maisel, and James P. McDermott, “The Optimum Strategy in Blackjack.” *Journal of the American Statistical Association* 51, no. 275 (1956): 429–439: 439.] For evaluating the equation and calculating the expected value of more complex situations (such as doubling or splitting hands), they hand-computed tables of conditional probabilities. According to Baldwin *et al.*, their strategy would result in an overall mathematical expectation of -0.006 as the net result of a single game.^[Baldwin *et al.*, "The Optimum Strategy", 439. To be more precise, -0.0062, according to Thorp in his paper as per the next footnote.] 

This strategy would later be further improved and popularized as the "basic strategy" by Edward O. Thorp in his book *Beat the Dealer*.^[The results were first partly published in Edward Thorp, “A Favorable Strategy for Twenty-One.” *Proceedings of the National Academy of Sciences - PNAS* 47, no. 1 (1961): 110–112, and later more broadly in Edward O. Thorp, *Beat the Dealer: a Winning Strategy for the Game of Twenty-One*. New York: Vintage Books, 1966.] Using Fortran and an IBM 704, Thorp computed which move had the greatest expected value in each situation. For Baldwin *et al.*'s strategy, he actually concluded on an expected value of -0.0021.^[Thorp, “A Favorable Strategy for Twenty-One”, 110. Later, in a footnote on page 16 of Thorp, *Beat the Dealer*, this went up further to result in a player advantage of 0.0009.] For his own basic strategy, he even concluded with a player advantage of 0.0013 for best play, but also stating that this "ranges from roughly -0.01 to +0.01".^[Thorp gives his values in percentages ("0.13; ranges from roughly -1 to +1"), but numbers here have been adjusted so that a 1 would always equal 100%, thus Baldwin's -0.0062 would be a -0.62%, and Thorp's values are shown as "0.0013, ranging from -0.01 to +0.01". For the full strategy, see chapter 3 of Thorp, *Beat the Dealer*, 16-40, and 32 for the expected values. For a short summary of Baldwin and Thorp's work on blackjack, also see: Wesley R. Gray and Tobias E. Carlisle, *Quantitative Value: a Practitioner’s Guide to Automating Intelligent Investment and Eliminating Behavioral Errors + Website*. 1st edition. Hoboken, New Jersey: Wiley, 2013, 5-6.]

Thorp's basic strategy is widely available online, often with various minor variations.^[For example: https://www.blackjackapprenticeship.com/blackjack-strategy-charts/ and https://www.vegashowto.com/blackjack-basic-strategy] This strategy is usually gathered in 3 small tables, one for player hard totals, one for player soft totals, and one specifically for hands that can be split (two cards of the same value, two 7's etc.), with the player hand shown in rows against the dealer's hand in columns. In the R code for this paper, these tables have been translated into look-up matrices, three of which are joined together in a list to constitute a whole strategy. Their full tables appear in the appendix. Values which are possible but fall outside of these tables can be assumed to be all stand when high, or all hit when low. "s" refers to stand, "h" to hit, "d" to double, "sp" to split, and "ds" to double if possible else stand. Thorp and Baldwin *et al.*'s strategies are almost identical, but Thorp doubles more often where Baldwin *et al.* hits.

One other strategy of the same family is an abbreviated version of the basic strategy, found online.^[https://blog.prepscholar.com/blackjack-strategy last consulted on 21 April 2023.] This is mostly similar to the basic strategy, but the interesting claim is made that "Compared to absolute perfect blackjack play, following these rules will only cost you about one hand in 12 hours of play". Assuming around 30 hands per hour, this would result in 360 hands per 12 hours, and one lost hand per 360 hands would thus translate to an expected value lower by around 0.00277. If practicality is a concern, then it might be worth giving up this much for a faster or more fun evening, assuming that the decrease in expected value isn't actually much more. 

The final strategy simply mirrors the dealer, hitting until the hand reaches a value of 17 or more. We use the dealer variation of S17, allowing to stand on a soft 17. According to Baldwin *et al.*, this strategy should have an expectation of -0.056, while according to Thorp the result should be -0.0573.^[Baldwin *et al.*, "The Optimum Strategy", 439; Thorp, *Beat the Dealer*, 38.] Given the near agreement between Thorp and Baldwin *et al.*, simulating this strategy also serves as a decent validation to see whether the programmed dealer logic performs as it should. 


# Code, Simulations, and Results

## Simulating a Game

The code is available through an R script, `play_blackjack.R`. This document itself was written in RMarkdown and can be reproduced locally, but just to be sure, the zip file with the code also includes a PDF version which doesn't hide the code chunks. Some inspiration was taken from Hadley Wickham's chapter 9, "Simulating Blackjack", of *Data Science in R*, but his examples lacked many options I wished to implement and the code soon diverged.^[Deborah Ann Nolan and David Temple Lang, *Data Science in R: a Case Studies Approach to Computational Reasoning and Problem Solving*. 1st edition. Boca Raton: CRC Press, 2015, chapter 9] For a more detailed description on how to use the code to simulate a game, see the accompanying `README.md`.

## Improvements

Currently, using multiple players works largely as it should, except for when a player splits. The way this is dealt with means that those two split hands veer off on their own, play a game with their dealer, report their result, and then the other players continue. This means that the dealer will draw different cards to determine the outcome of those split hands compared to the hands of the other players. This isn't a problem for the simulations below because those only consider a single player, but a multiplayer extension of this code needs to be aware of this. 



```{r}
# Test for hand_value function
test_cards = list(c(10, 1), c(10, 5, 6), c(10, 1, 1),
    c(7, 6, 1, 5), c(3, 6, 1, 1),
    c(2, 3, 4, 10), c(5, 1, 9, 1, 1),
    c(5, 10, 7), c(10, 9, 1, 1, 1))
test_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0)
test_1 <- identical(test_cards_val, sapply(test_cards, hand_value))

# Test for game_outcome function
test_vals = c(0, 16, 19, 20, 21, 21.5)
test_winnings =
 matrix(c(
   -1, 1, 1, 1, 1, 1.5,
   -1, 0, 1, 1, 1, 1.5,
   -1, -1, 0, 1, 1, 1.5,
   -1, -1, -1, 0, 1, 1.5,
   -1, -1, -1, -1, 0, 1.5,
   -1, -1, -1, -1, -1, 0),
   nrow = length(test_vals), byrow = TRUE)
dimnames(test_winnings) = list(dealer = test_vals,
      player = test_vals)

check = test_winnings
check[] = NA
for(i in seq_along(test_vals)) {
  for(j in seq_along(test_vals)) {
 check[i, j] = game_outcome(test_vals[j], test_vals[i])
 }
}
test_2 <- identical(check, test_winnings)

stopifnot(all(test_1, test_2))
```


## Testing and Validation

Two tests were borrowed from chapter 9 of *Data Science in R* to make sure some minor functions perform properly, namely `hand_value` and `game_outcome`.^[Nolan and Lang, *Data Science in R*, chapter 9.2] Other tests involved checking the output for impossible values, such as dealer hands that didn't go bust but were below 17. 

We can use the `dealer_plays` function to solely simulate dealer hands. This allows to validate whether the resulting hands align with our expectations, and whether a rule variation of S17 or H17 shows a difference. Thorp includes a table for the dealer probabilities when standing on a soft 17, the percentages of which are reproduced below as well.^[Thorp, *Beat the Dealer*, 189.] 

```{r}
# Estimated time to run: 50 seconds
# Indep option makes sure dealer draws from a fresh deck each time

games_to_run <- 300000
set.seed(39486)
outcomes_1 <- data.frame(Outcomes = replicate(games_to_run, 
                                              dealer_plays(S_17 = FALSE,
                                                           indep = TRUE)),
                         Rule = "Hit on Soft 17")
set.seed(39486)
outcomes_2 <- data.frame(Outcomes = replicate(games_to_run, 
                                              dealer_plays(S_17 = TRUE,
                                                           indep = TRUE)),
                         Rule = "Stand on Soft 17")
dealer_outcomes <- outcomes_1 %>% 
  bind_rows(outcomes_2)

```

```{r}
hit_on_soft <- dealer_outcomes[(dealer_outcomes$Rule == "Hit on Soft 17"), ]$Outcomes
stand_on_soft <- dealer_outcomes[dealer_outcomes$Rule == "Stand on Soft 17", ]$Outcomes

table1 <- tabyl(hit_on_soft, sort = TRUE) %>% 
  rename("Hit on Soft 17" = hit_on_soft)
table2 <- tabyl(stand_on_soft, sort = TRUE) %>% 
  rename("Stand on Soft 17" = stand_on_soft)

table3 <- data.frame(Thorp = c(0, 17.0, 18.0, 19.0, 20.0, 21.0, 21.5),
                       percent = c(0.2836, 0.1458, 0.1381, 0.1348, 
                                   0.1758, 0.0736, 0.0483))

kable(list(table1, table2, table3),
      caption = 'Dealer Hand Values (n=300,000)',
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


```{r}
chi_test_stat <- table2 %>% 
  left_join(table3, by = c("Stand on Soft 17" = "Thorp")) %>% 
  mutate(percent.y = percent.y * games_to_run) %>% 
  mutate(diff = (n - percent.y) / percent.y) %>% 
  summarise(test_stat = sum(diff)) %>% 
  pull(test_stat)

crit_val <- qchisq((1 - 0.01), (7 - 1 - 1), lower.tail=TRUE)

stopifnot(chi_test_stat < crit_val)
```

Running `r format(games_to_run, scientific=FALSE, big.mark = ",")` dealer hands, the outcomes are shown above. Using Thorp's figures as expected values for stand on 17, we can perform a chi-squared goodness of fit test. The resulting test statistic is tiny (`r chi_test_stat`), and considering the critical value of `r round(crit_val, 4)`, we fail to reject $H_0$ for our simulation results being different from Thorp's values.

While there is a difference between the S17 and H17 variations, it is too small to be statistically significant. But they do behave as expected, with a decrease in hand totals of 17 when hitting on a soft 17, more hands going bust, and a slight increase for all values above 17. 


```{r, fig.height=4}
ggplot(dealer_outcomes, aes(x=Outcomes, color=Rule)) +
  geom_histogram(fill="white", alpha=0.5, position="identity", binwidth = 1) +
  xlab("Dealer Hand Values") +
  ggtitle("Chart 1: Histogram of Dealer Hands for S17 and H17 (n=300,000)") +
  theme_minimal()
```


## Simulating Many Games

The simulations below were all performed using the `run_ir_games` function to create independent replications of many blackjack games. The function reshuffles the deck before each new game, making each game independent, but the player and dealer hands within a game are not independent. The first replication of m observations begins with a given seed value to create a reproducible output, and each subsequent replication adds 1 to this seed to make sure that the replications themselves are independent. This is most likely redundant, given that the games themselves are independent and R uses the Mersenne Twister algorithm for whenever a new deck is sampled (shuffled), but it also comes at little extra computational cost. See the `README.md` file on how to run this function and its inputs.

The output consists of 2 values for each game: the outcome and the bet. The outcome is either -1 for a lost game, 0 for a tie, 1 for a win, or 1.5 for a win with a natural 21. The bet values are either 1 for the initial bet or 2 for a double. Each split hand creates 2 results, thus each replication limits the output to only include the number of observations requested per replication. The net result for each game is simply the game outcome multiplied by the bet value. The mean of these net results are the expected value for a strategy per initial bet of 1 currency unit. 


## Strategy Simulations

All strategies were simulated with 100 runs and 5,000 observations per run for a total of 500,000 observations per strategy. At an alpha of 0.01, this produces fairly small confidence intervals while keeping simulation run times reasonable. Other configurations were tested but increasing observations or replications further only marginally decreased the confidence intervals compared to the chosen settings.^[With a few million observations these tests took rather long to run and were left out of this paper.] 


```{r}
# Each simulation took around 2 minutes to run
# There are 4 simulations below
# So if you run this chunk, expect to wait roughly 8 minutes

r_runs <- 100
m_obs <- 5000
alpha <- 0.01
  
df_1 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_1,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_1 <- get_ir_sample_means(df_1)
z_bars_1 <- get_ir_grand_sample_mean(get_ir_sample_means(df_1))
sample_vars_1 <- get_ir_sample_variance(get_ir_sample_means(df_1))
cis_1 <- get_ir_cis_full(z_bars = z_bars_1,
                       sample_vars = sample_vars_1,
                       r_runs = r_runs,
                       alpha = alpha)

df_2 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_2,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_2 <- get_ir_sample_means(df_2)
z_bars_2 <- get_ir_grand_sample_mean(get_ir_sample_means(df_2))
sample_vars_2 <- get_ir_sample_variance(get_ir_sample_means(df_2))
cis_2 <- get_ir_cis_full(z_bars = z_bars_2,
                       sample_vars = sample_vars_2,
                       r_runs = r_runs,
                       alpha = alpha)

df_3 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_3,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_3 <- get_ir_sample_means(df_3)
z_bars_3 <- get_ir_grand_sample_mean(get_ir_sample_means(df_3))
sample_vars_3 <- get_ir_sample_variance(get_ir_sample_means(df_3))
cis_3 <- get_ir_cis_full(z_bars = z_bars_3,
                       sample_vars = sample_vars_3,
                       r_runs = r_runs,
                       alpha = alpha)

df_4 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_4,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_4 <- get_ir_sample_means(df_4)
z_bars_4 <- get_ir_grand_sample_mean(get_ir_sample_means(df_4))
sample_vars_4 <- get_ir_sample_variance(get_ir_sample_means(df_4))
cis_4 <- get_ir_cis_full(z_bars = z_bars_4,
                       sample_vars = sample_vars_4,
                       r_runs = r_runs,
                       alpha = alpha)

kable(list(cis_4, 
           cis_3 %>% 
             remove_rownames(),
           cis_1,
           cis_2 %>% 
             remove_rownames()),
      caption = 'Strategy Means and CIs: Mimic Dealer, Simplified, Baldwin, Thorp',
      booktabs = TRUE,
      valign = 't',
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


The first general result to be noted is the accuracy when compared to Baldwin *et al.*'s results from 1956. The simulated mean of `r round(cis_1[2, 3], 6)` comes in slightly above their own -0.0062, but their figure lies safely within the `r paste0(((1 - alpha) * 100), "%")` confidence interval of `r paste0("(", paste0(round(cis_1[c(1, 3), 3], 6), collapse = ", "), ")")`. Their calculated result for mimicking the dealer of -0.056 was also spot on, while Thorp's -0.0573 seems slightly too pessimistic but also falls squarely within the `r paste0(((1 - alpha) * 100), "%")` confidence interval of `r paste0("(", paste0(round(cis_4[c(1, 3), 3], 6), collapse = ", "), ")")`. The bet value for mimicking the dealer is also always 1, which is the expected value when a player never splits or doubles on an initial bet of 1, making the net results identical to the game outcomes. 

Thorp's improvements do seem to lead to a slightly better net result, but the simulation results don't match Thorp's own numbers quite as well. The simulated results are on the low side, but the rather large range of +0.01 to -0.01 for Thorp's expected value of 0.0013 makes comparison difficult if not impossible, particularly as more detailed information is lacking. In Thorp's book, percentage expectations are thrown about with wild abandon for a great variety of blackjack strategies, for rule variations, and even for completely different games, all with nary a variance or confidence interval in sight. Statistical examples given are generally small, vary greatly, and lack detailed information.^[For example Thorp, *Beat the Dealer*, 31-34, 46-48, including an expected value for US roulette ranging from -0.027 to -0.0526. Baldwin *et al.* do provide a variance of 1.1 for their expectation of -0.0062, but give no further details on how this was computed.] 

However, there are some small differences in assumptions and method between Baldwin *et al.*, Thorp, and our own simulations above, not all of which could be brought into congruence and which could lead to different results. For one, Thorp's basic strategy comes with a number of exceptions where it is unclear whether they are part of the basic strategy or not. These depend on specific combinations of cards within the matrices, for example standing with two 7's against a dealer 10, listed as a "refinement" of the basic strategy.^[There are 5 such refinements, marked with stars and crosses in Thorp's tables, see Thorp, *Beat the Dealer*, 29-31.] For simplicity these have been left out here, and their effect on the net results is thus unclear. Another difference has been mentioned earlier in relation to Baldwin *et al.*, which is the assumption of independence between hands during a game. This is generally mathematically convenient but unrealistic, and the simulated games here draw from a common deck instead, without replacement until the next game.^[For example, Thorp, *Beat the Dealer*, 37, on dealer probabilities and dealer mimicking strategies, where Thorp assumes "stochastic independence, not strictly valid but good in this instance to a high degree of approximation when the deck is nearly complete".] How these different assumptions and exceptions affect the outcome could be explored further. 

What about the claim that the simplified strategy "will only cost you about one hand in 12 hours of play"? Assuming that this difference of one hand in 12 hours is equal to the half length of a confidence interval, that 1 hand in 12 hours at 30 per hour (rather slow, worst case) translates to 1 lost hand out of 360 hands, resulting in an expected value difference of around -0.002777, and that the simulation results from Thorp's basic strategy equals "absolute perfect blackjack play", we can check whether the difference between the means of the simulated results from the simplified strategy and Thorp's strategy allows for this difference of -0.00277 or not.


```{r}
simp_vs_thorp <- get_mean_diff_ci(x_bar = z_bars_3$net_mean,
                                  y_bar = z_bars_2$net_mean,
                                  alpha = 0.05,
                                  s_var_x = sample_vars_3$net_variance,
                                  s_var_y = sample_vars_2$net_variance,
                                  n = r_runs,
                                  m = r_runs)
```


The code currently does not support common random numbers and we will have to do with an approximate confidence interval for a difference in means.^[As each simulation uses the same starter seed for its replications, it begins a run using common random numbers, but only until a difference in strategy leads to a different number of cards being drawn, after which they diverge until the next run is started. Because there is so little difference between Baldwin *et al.* and Thorp, this does still result in identical outcomes for `r round(sum(df_1[, 1] == df_2[, 1]) / (r_runs * m_obs), 4) * 100`% of all simulated games between Baldwin *et al.* and Thorp, but it is not rigorous enough to assume CRNs.] Given an alpha of 0.05, the confidence interval for the difference between the two means is `r paste0("(", paste(round(simp_vs_thorp[c(1, 3)], 6), collapse = ", "), ")")`, with a mean difference of `r round(simp_vs_thorp[c(2)], 6)`. First, seeing as how this is entirely negative, Thorp's strategy has to be assumed to be better than the simplified version. 

Second, while -0.002777 falls within this confidence interval, the lower bound is actually lower than -0.002777. This means that the claim that this simplified strategy will cost at most 1 hand in 12 hours against "perfect" blackjack will have to be rejected. Assuming 360 hands in 12 hours, the worst additional loss compared to Thorp at a 95% confidence interval is `r abs(round(simp_vs_thorp[c(1)] * 360, 6))` hands or `r ceiling(abs(round(simp_vs_thorp[c(1)] * 360, 6)))` when rounded up. Still, if you think it is acceptable to play with an expectation lower by maximally `r round(simp_vs_thorp[c(1)], 6)` (with a confidence level of 95%) compared to Thorp's strategy - or in blackjack parlance, lose at most `r ceiling(abs(round(simp_vs_thorp[c(1)] * 360, 6)))` more hand in 12 hours of play - then a simplified strategy such as the one included here might be worth using.

```{r}
baldwin_vs_thorp <- get_mean_diff_ci(x_bar = z_bars_1$net_mean,
                                  y_bar = z_bars_2$net_mean,
                                  alpha = 0.05,
                                  s_var_x = sample_vars_1$net_variance,
                                  s_var_y = sample_vars_2$net_variance,
                                  n = r_runs,
                                  m = r_runs)
```


\vspace{12pt}

```{r}
strat_sample_means <- get_ir_sample_means(df_1) %>% 
  mutate(Strategy = "Baldwin") %>%
  bind_rows(
    get_ir_sample_means(df_2) %>% 
      mutate(Strategy = "Thorp"),
    get_ir_sample_means(df_3) %>% 
      mutate(Strategy = "Simplified"),
    get_ir_sample_means(df_4) %>% 
      mutate(Strategy = "Mimic Dealer"))

ggplot(strat_sample_means, aes(x=net_means, color=Strategy)) +
  geom_histogram(alpha=0.2, position="identity", binwidth = 0.005) +
  scale_color_manual(values=c("#FFFF00", "#CC3300", "#00CC00", "#0000FF")) +
  scale_x_continuous(breaks = round(seq(min(strat_sample_means$net_means), 
                                        max(strat_sample_means$net_means), 
                                        by = 0.005), 2)) +
  theme_minimal() +
  xlab("Net Results") +
  ggtitle("Chart 2: Histograms of Strategy Sample Means of Net Results")
```

Which of these strategies is the most profitable? Given that mimicking the dealer is so much worse than the other three strategies, this strategy won't be taken into further consideration. Above, the simplified strategy has also already proven to be statistically worse than Thorp's, which leaves us with Baldwin *et al.* against Thorp. Optimistically, the upper bounds for both Baldwin *et al.*'s and Thorp's strategy do fall into the positive. From their net result confidence intervals alone they look nearly indistinguishable, and graphically as in chart 2 they are also almost identical. Given an alpha of 0.05, the confidence interval for the difference between their two means is `r paste0("(", paste(round(baldwin_vs_thorp[c(1, 3)], 6), collapse = ", "), ")")`, with a mean difference of `r format(round(baldwin_vs_thorp[c(2)], 6), scientific = FALSE)`. With 0 being within this confidence interval, we can't reject that they are equal and the result is thus inconclusive. Interestingly, the worst expected difference at 360 hands per 12 hours would even here be `r abs(round(baldwin_vs_thorp[c(1)] * 360, 2))` additional lost hands. Clearly, remarks on hands lost per duration of play shouldn't be made lightly.

Finally, it is worth mentioning the other means shown earlier, for game outcomes and bet sizes. One result that is true across all 4 strategies, is that, as the strategies become more successful, both the outcome and the bet size increases. This can also be explored graphically in charts 3 and 4 below. If we reduce the outcomes to -1 for a loss, 0 for a draw, and 1 for a win, then the column wise table of observed probabilities per bet size for Thorp is as shown below. 

While a strategy for bets of size 1 look like it should be mostly damage control, the real difference is being made with bets of a higher value. This hints at the idea that, not only is it important to win as often as possible, but also to bet as high as possible when chances are best, and that the strategy differences for doubling are particularly important. This idea was much more thoroughly utilized in the method of card counting for consecutive games on single decks, where the knowledge of a deck and thus its conditional probabilities would have to be updated after each visible card until the next shuffle. However, that would require a simulation of dependent games, which is not part of the current scope.


```{r}
round(prop.table(xtabs(~Outcomes + `Bet Size`,
                       data= df_2 %>% 
                         mutate(game_outcomes = case_when(
                           game_outcomes == 0 ~ 0,
                           game_outcomes == -1 ~ -1,
                           game_outcomes > 0 ~ 1)) %>% 
                         rename("Bet Size" = "bets",
                                "Outcomes" = "game_outcomes")), 2), 6) %>% 
  kable(caption = 'Thorp Strategy: Outcome Probability per Bet Size',
      booktabs = TRUE,
      valign = 't',
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```

```{r}

chart_3 <- ggplot(strat_sample_means, aes(x=outcome_means, color=Strategy)) +
  geom_histogram(alpha=0.2, position="identity", binwidth = 0.005) +
  scale_color_manual(values=c("#FFFF00", "#CC3300", "#00CC00", "#0000FF")) +
  scale_x_continuous(breaks = round(seq(min(strat_sample_means$outcome_means), 
                                        max(strat_sample_means$outcome_means), 
                                        by = 0.005), 2)) +
  theme_minimal() +
  xlab("Game Outcomes") +
  ggtitle("Chart 3: Histograms of Strategy Sample Means of Game Outcomes")

chart_4 <- ggplot(strat_sample_means, aes(x=bet_means, color=Strategy)) +
  geom_histogram(alpha=0.2, position="identity", binwidth = 0.005) +
  scale_color_manual(values=c("#FFFF00", "#CC3300", "#00CC00", "#0000FF")) +
  scale_x_continuous(breaks = round(seq(min(strat_sample_means$bet_means), 
                                        max(strat_sample_means$bet_means), 
                                        by = 0.005), 2)) +
  theme_minimal() +
  xlab("Bet Sizes") +
  ggtitle("Chart 4: Histograms of Strategy Sample Means of Bet Sizes")

chart_3 /
  chart_4
```




# Conclusions




\newgeometry{left=0.2in,right=0.2in,top=0.2in,bottom=0.2in}

# Appendix

## Strategy Tables


```{r}

kable(list(lb_h1[6:14, ], lb_s1[-1,], lb_sp1), 
      caption = "Baldwin et al.'s Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```



```{r}
kable(list(lb_h2[6:14, ], lb_s2[-1,], lb_sp2), 
      caption = "Thorp's Basic Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```

```{r}
kable(list(lb_h3[6:14, ], lb_s3[-1,], lb_sp3), 
      caption = "Simplified Basic Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


```{r}
kable(list(lb_h4[6:14, ], lb_s4[-1,], lb_sp4), 
      caption = "Mimic Dealer (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


\restoregeometry









