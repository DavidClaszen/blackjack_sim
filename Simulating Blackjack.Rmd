---
title: "ISYE 6644 Simulation: To Hit or Not to Hit, That is the Question"
author: "Group 162: David Claszen"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "Output") })
---

```{r setup, include=FALSE}
# I have included an estimated run time for taxing code chunks. 
# These are from my relatively rubbish laptop, so
# as long as you're not using a potato, this should run faster

knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(janitor)
library(kableExtra)
source("play_blackjack.R")
```


# Abstract

Blackjack is a well-known casino card game that has captured the attention of many players hoping to beat the house through various strategies. A brief overview of the game is given, followed by a literature review and explanation of multiple strategies. Code was developed in R to simulate blackjack games and strategies, which is explained, tested, and validated. Using the simulation outputs, I explore and compare the profitability of 4 strategies for independent games: Baldwin’s basic strategy, an improvement on the basic strategy by Thorp, a simplified version of this basic strategy, and mimicking the dealer. 

# Background and Description of Problem

## Blackjack

Blackjack is a gambling casino card game with around 2 to 7 players using one or multiple 52 card decks. Each round, the players place an initial bet and are then dealt 2 cards each by the dealer. The dealer deals him/herself two cards as well, the first of which visible, face-up, and the second face down (the hole card). The goal is to win money by creating hands with a value higher than the dealer's hand but not exceeding 21, which would count as a loss or bust. Each player gets to decide whether to "hit" and get another card, to "double" their bet and get one final card and finish, to "split" their current hand into two new hands (only for cards with same value and requiring another bet as well), or to "stand" and finish their turn with their current hand. After the players have finished their hands, the dealer draws until their hand achieves a total of 17 or higher, but never doubles or splits. If the dealer busts, all players that didn't bust win an amount equal to their bet, but players who already went bust lose even if the dealer busts. If the player and dealer tie (a "push"), the bet is returned. Other possible moves which aren't considered below are the ability to surrender (losing only half your bet), or to buy insurance when the dealer's face-card is an ace.

All cards with a numerical value (2 to 10) are worth as many points as their number. All face cards (kings, queens, etc.) are worth 10 points. An ace can be worth either 1 or 11 points depending on whether counting it as 11 would push the total beyond 21 or not. A natural happens when an initial hand consists of an ace and a face card, totaling 21. If only the player holds a natural, he receives 1.5 times his original bet. If both the player and dealer hold a natural, it is another push and the bet is returned. One mayor rule variation is whether the dealer has to hit or stand on a soft 17. A soft hand is a hand with an ace that can be counted as 11 without the total exceeding 21; that is, the ace can still be counted as a 1 if further draws push the total beyond 21. The dealer having to stand on soft 17 is commonly referred to as S17 and having to hit as H17. 

## Blackjack Strategies

The main problem is the simulation and analysis of multiple blackjack strategies and to determine which choice maximizes profit. The player decision whether to stand or hit, given a hand of cards and a visible dealer card, had already been solved mathematically since Baldwin *et al.*'s 1956 paper, "The Optimum Strategy in Blackjack".^[Roger R. Baldwin, Wilbert E. Cantey, Herbert Maisel, and James P. McDermott, “The Optimum Strategy in Blackjack.” *Journal of the American Statistical Association* 51, no. 275 (1956): 429–439: 439.] For evaluating the equation and calculating the expected value of more complex situations (such as doubling or splitting hands), they hand-computed tables of conditional probabilities. According to Baldwin *et al.*, their strategy would result in an overall mathematical expectation of -0.006 as the net result of a single game.^[Baldwin *et al.*, "The Optimum Strategy", 439. To be more precise, -0.0062, according to Thorp in the paper as per the next footnote.] 

This strategy would later be further improved and popularized as the "basic strategy" by Edward O. Thorp in his book *Beat the Dealer*.^[The results were first partly published in Edward Thorp, “A Favorable Strategy for Twenty-One.” *Proceedings of the National Academy of Sciences - PNAS* 47, no. 1 (1961): 110–112, and later more broadly, but statistically less rigorous, in Edward O. Thorp, *Beat the Dealer: a Winning Strategy for the Game of Twenty-One*. New York: Vintage Books, 1966.] Using Fortran and an IBM 704, Thorp computed which move had the greatest expected value in each situation. For Baldwin *et al.*'s strategy, he actually concluded on an expected value of -0.0021.^[Thorp, “A Favorable Strategy for Twenty-One.”, 110. Later, in a footnote on page 16 of Thorp, *Beat the Dealer*, this went up even further to result in a player advantage of 0.0009.] For his own basic strategy, he even concluded with a player advantage of 0.0013 for best play, but also stating that this "ranges from roughly -0.01 to +0.01".^[Thorp gives his values in percentages ("0.13; ranges from roughly -1 to +1"), but numbers here have been adjusted so that a 1 would always equal 100%, thus Baldwin's -0.006 would be a -0.6%, and Thorp's values are shown as "0.0013, ranging from -0.01 to +0.01". For the full strategy, see chapter 3 of Thorp, *Beat the Dealer*, 16-40, and 32 for the expected values. For a short summary of Baldwin and Thorp's work on blackjack, also see: Wesley R. Gray and Tobias E. Carlisle, *Quantitative Value: a Practitioner’s Guide to Automating Intelligent Investment and Eliminating Behavioral Errors + Website*. 1st edition. Hoboken, New Jersey: Wiley, 2013, 5-6.]

Thorp's basic strategy is widely available online, often with various minor variations.^[For example: https://www.blackjackapprenticeship.com/blackjack-strategy-charts/ and https://www.vegashowto.com/blackjack-basic-strategy] This strategy is usually gathered in 3 small tables, one for player hard totals, one for player soft totals, and one specifically for hands that can be split (two cards of the same value, two 7's etc.), with the player hand shown in rows against the dealer's hand in columns. In the R code for this paper, these tables have been translated into look-up matrices, three of which are joined together in a list to constitute a whole strategy. Their full tables appear in the appendix. Values which are possible but fall outside of these tables can be assumed to be all stand when high, or all hit when low. "s" refers to stand, "h" to hit, "d" to double, "sp" to split, and "ds" to double if possible else stand. Thorp and Baldwin *et al.*'s strategies are almost identical, but Thorp doubles more often where Baldwin *et al.* hits.

One other strategy of the same family is an abbreviated version of the basic strategy, found online.^[https://blog.prepscholar.com/blackjack-strategy last consulted on 21 April 2023.] This is mostly similar to the basic strategy, but the interesting claim is made that "Compared to absolute perfect blackjack play, following these rules will only cost you about one hand in 12 hours of play". Assuming around 30 hands per hour, this would result in 360 hands per 12 hours, and one lost hand per 360 hands would thus translate to an expected value lower by around 0.00277. If practicality is a concern, then it might be worth giving up this much for a faster or more fun evening, assuming that the decrease in expected value isn't actually much more. 

The final strategy simply mirrors the dealer, hitting until the hand reaches a value of 17 or more. We use the dealer variation of S17, allowing to stand on a soft 17. According to Baldwin *et al.*, this strategy should have an expectation of -0.056, while according to Thorp the result should be -0.0573.^[Baldwin *et al.*, "The Optimum Strategy", 439; Thorp, *Beat the Dealer*, 38.] Given the agreement between Thorp and Baldwin *et al.*, simulating this strategy also serves as a decent validation to see whether the programmed dealer logic performs as it should. 


# Code

## Simulating a Game

The code is available through an R script but also shown in the appendix. This document itself was written in RMarkdown and can be reproduced locally, but just to be sure, the zip file with the code also includes a PDF version which doesn't hide the code chunks. Some inspiration was taken from Hadley Wickham's chapter 9, "Simulating Blackjack", of *Data Science in R*, but his examples lacked many options I wished to implement and the code soon diverged.^[Deborah Ann Nolan and David Temple Lang, *Data Science in R: a Case Studies Approach to Computational Reasoning and Problem Solving*. 1st edition. Boca Raton: CRC Press, 2015, chapter 9]

In broad strokes, the code does the following. `shuffle_deck` creates n-decks, shuffles them, and stores the result in the global environment. If n-decks equals 1, and each game is set to require 52 cards, the deck would be shuffled before each game and the games (not the hands) would be independent. Alternatively, initializing only 1 deck and delaying the shuffle would make games in between each shuffle correlated, more closely approximating a casino from before automatic shufflers and multideck blackjack. This functionality was intended for an expansion into card counting that didn't make it into the final paper.

The `play_game` function calls on all other required functions so as to simulate one game of blackjack. `play_game` first deals 2 cards to the dealer, creates as many players as requested, places their initial bets, and deals them 2 cards using `deal_cards` which it takes and removes from the global deck. Differences between the simulation and other strategy results might derive from this, because calculations such as by Baldwin often allow for an approximation based on a deck that is always full (sampling with replacement), rather than taking out cards as a single game progresses.^[For example, Baldwin *et al.*, "The Optimum Strategy", 434, their second assumption for dealer probabilities: "no matter how many cards the player draws, the probability of receiving any particular card on the next draw is still 1/52 ("sampling with replacement")".]

Then, each player action is decided on through logical operators and look-up tables in the function `player_logic`. Each strategy consists of a list of three look-up tables (one for hard totals, one for soft totals, and one for splits), where the recommended action is looked up from rows of player hand values and columns of dealer face card values. `player_actions` executes the taken action: either to stand, hit, double, or split. The resulting hand then goes back to `play_game`. 

If the player didn't go bust already, `dealer_logic` runs through the fixed rules for a dealer to determine their final hand. Based on that value, and the final player hand, the outcome is determined by `game_outcome` and returned. To distinguish it from a normal 21, a natural blackjack counts as 21.5. The initial bet is always 1. 

In the case of a split, the hand is split into two new hands, and each hand is given an additional card within `player_actions`. Together with the existing dealer hand, these two hands are send back to the `play_game` function to determine their outcome, and then averaged as the result for that game. There is also a manual option to play some games of blackjack against the dealer based on your input actions. This wasn't the focus of this project and hasn't been tested thoroughly, but it works well enough.

After loading the script with `source("play_blackjack.R")`, a single game simulation can be called upon through for example `play_game(num_players = 1, initial_bet = 1, manual = FALSE, S_17 = TRUE, logic_board = lb_1)`, where:

- num_players (integer): Specifies the number of players, default 1.
- initial_bet (integer): Specifies the initial bet, default 1.
- manual (boolean): Switches between manual games and automatic, default FALSE. If manual equals FALSE, the output will be a list of values, with the first value the outcome of the game and the second the value of the bet.
- S_17 (boolean): Switches between S17 (TRUE) and H17 (FALSE), default TRUE.
- logic_board (list of three matrices): Input for the required list of three matrices to follow a strategy.



## Improvements

Currently, using multiple players works largely as it should, except for when a player splits. The way this is dealt with means that those two split hands veer off on their own, play a game with their dealer, report their average result, and then the other players continue. This means that the dealer will draw different cards to determine the outcome of those split hands compared to the hands of the other players. This isn't a problem for the simulations below because those only consider a single player, but a multiplayer extension of this code needs to be aware of this. 



```{r}
# Test for hand_value function
test_cards = list(c(10, 1), c(10, 5, 6), c(10, 1, 1),
    c(7, 6, 1, 5), c(3, 6, 1, 1),
    c(2, 3, 4, 10), c(5, 1, 9, 1, 1),
    c(5, 10, 7), c(10, 9, 1, 1, 1))
test_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0)
test_1 <- identical(test_cards_val, sapply(test_cards, hand_value))

# Test for game_outcome function
test_vals = c(0, 16, 19, 20, 21, 21.5)
test_winnings =
 matrix(c(
   -1, 1, 1, 1, 1, 1.5,
   -1, 0, 1, 1, 1, 1.5,
   -1, -1, 0, 1, 1, 1.5,
   -1, -1, -1, 0, 1, 1.5,
   -1, -1, -1, -1, 0, 1.5,
   -1, -1, -1, -1, -1, 0),
   nrow = length(test_vals), byrow = TRUE)
dimnames(test_winnings) = list(dealer = test_vals,
      player = test_vals)

check = test_winnings
check[] = NA
for(i in seq_along(test_vals)) {
  for(j in seq_along(test_vals)) {
 check[i, j] = game_outcome(test_vals[j], test_vals[i])
 }
}
test_2 <- identical(check, test_winnings)

stopifnot(all(test_1, test_2))
```


## Testing and Validation

Two tests were borrowed from chapter 9 of *Data Science in R* to make sure some minor functions perform properly, namely `hand_value` and `game_outcome`.^[Nolan and Lang, *Data Science in R*, chapter 9.2] Other tests involved checking the output for impossible values, such as dealer hands that didn't go bust but were below 17. 

We can use the `dealer_plays` function to solely simulate dealer hands. This allows to validate whether the resulting hands align with our expectations, and whether a rule variation of S17 or H17 shows a difference. Thorp includes a table for the dealer probabilities when standing on a soft 17, the percentages of which are reproduced below as well.^[Thorp, *Beat the Dealer*, 189.] 

```{r}
# Estimated time to run: 50 seconds
# Indep option makes sure dealer draws from a fresh deck each time

games_to_run <- 300000
set.seed(39486)
outcomes_1 <- data.frame(Outcomes = replicate(games_to_run, 
                                              dealer_plays(S_17 = FALSE,
                                                           indep = TRUE)),
                         Rule = "Hit on Soft 17")
set.seed(39486)
outcomes_2 <- data.frame(Outcomes = replicate(games_to_run, 
                                              dealer_plays(S_17 = TRUE,
                                                           indep = TRUE)),
                         Rule = "Stand on Soft 17")
dealer_outcomes <- outcomes_1 %>% 
  bind_rows(outcomes_2)

```

```{r}
hit_on_soft <- dealer_outcomes[(dealer_outcomes$Rule == "Hit on Soft 17"), ]$Outcomes
stand_on_soft <- dealer_outcomes[dealer_outcomes$Rule == "Stand on Soft 17", ]$Outcomes

table1 <- tabyl(hit_on_soft, sort = TRUE) %>% 
  rename("Hit on Soft 17" = hit_on_soft)
table2 <- tabyl(stand_on_soft, sort = TRUE) %>% 
  rename("Stand on Soft 17" = stand_on_soft)

table3 <- data.frame(Thorp = c(0, 17.0, 18.0, 19.0, 20.0, 21.0, 21.5),
                       percent = c(0.2836, 0.1458, 0.1381, 0.1348, 
                                   0.1758, 0.0736, 0.0483))

kable(list(table1, table2, table3),
      caption = 'Dealer Hand Values (n=300,000)',
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


```{r}
chi_test_stat <- table2 %>% 
  left_join(table3, by = c("Stand on Soft 17" = "Thorp")) %>% 
  mutate(percent.y = percent.y * games_to_run) %>% 
  mutate(diff = (n - percent.y) / percent.y) %>% 
  summarise(test_stat = sum(diff)) %>% 
  pull(test_stat)

crit_val <- qchisq((1 - 0.01), (7 - 1 - 1), lower.tail=TRUE)

stopifnot(chi_test_stat < crit_val)
```

Running `r format(games_to_run, scientific=FALSE)` dealer hands, the outcomes are shown above. Using Thorp's figures as expected values for stand on 17, we can perform a chi-squared goodness of fit test. The resulting test statistic is tiny (`r chi_test_stat`), and considering the critical value of `r round(crit_val, 4)`, we fail to reject $H_0$ for our simulation results being different from Thorp's values.

\vspace{12pt}

```{r, fig.height=4}
ggplot(dealer_outcomes, aes(x=Outcomes, color=Rule)) +
  geom_histogram(fill="white", alpha=0.5, position="identity", binwidth = 1) +
  xlab("Dealer Hand Values") +
  ggtitle("Chart 1: Histogram of Dealer Hands for S17 and H17 (n=300,000)") +
  theme_minimal()
```

\vspace{12pt}

While there is a difference between the S17 and H17 variations, it is too small to be statistically significant. But they do behave as expected, with a decrease in hand totals of 17 when hitting on a soft 17, more hands going bust, and a slight increase for all values above 17. 


## Simulating Many Games

The simulations below were all performed using the `run_ir_games` function to create independent replications of many blackjack games. The function reshuffles the deck before each new game, making each game independent, but the player and dealer hands within a game are not independent. The first replication of m observations begins with a given seed value to create a reproducible output, and each subsequent replication adds 1 to this seed to make sure that the replications themselves are independent. This is most likely redundant, given that the games themselves are independent and R uses the Mersenne Twister algorithm for whenever a new deck is sampled (shuffled), but it also comes at little extra computational cost. The inputs for the function are as follows:

- r_runs (integer): The number of replications.
- m_obs (integer): The number of observations per replication.
- S_17 (boolean): Switches between S17 (TRUE) and H17 (FALSE), default TRUE. 
- logic_board (list of three matrices): Input for the required list of three matrices to follow a strategy.
- seed (integer): Starting seed value to ensure reproducible output.
- required_cards (integer): Optional, defaults to 52. Cutoff for minimum remaining cards in deck before reshuffling occurs.

The output consists of 2 values for each game: the outcome and the bet. The outcome is either -1 for a lost game, 0 for a tie, 1 for a win, or 1.5 for a win with a natural 21. The bet values are either 1 for the initial bet, 2 for a double, and very occasionally 1.5 for when a hand was split and the resulting hands produced a normal bet of 1 and 2 for a double. The net result for each game is simply the game outcome multiplied by the bet value. The mean of these net results are the expected value for a strategy per initial bet of 1 currency unit. 


## Strategy Simulations

All strategies were simulated with 100 runs and 5000 observations per run for a total of 500,000 observations per strategy. At an alpha of 0.01, this produces fairly small confidence intervals while keeping simulation run times reasonable. Other configurations were tested but increasing observations or replications further only marginally decreased the confidence intervals compared to the chosen settings.^[With a few million observations these tests took rather long to run and were left out of this paper.] The functions `get_ir_sample_means`, `get_ir_grand_sample_mean`, `get_ir_sample_variance`, `get_ir_cis_full`, do what you would expect, the first taking as input the output from `run_ir_games`, the next two requiring the output from `get_ir_sample_means`, and the last function combines the information of the previous functions together with an alpha to produce confidence intervals. 


```{r}
# Each simulation took around 2 minutes to run
# There are 4 simulations below
# So if you run this chunk, expect to wait roughly 8 minutes

r_runs <- 100
m_obs <- 5000
alpha <- 0.01
  
df_1 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_1,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_1 <- get_ir_sample_means(df_1)
z_bars_1 <- get_ir_grand_sample_mean(get_ir_sample_means(df_1))
sample_vars_1 <- get_ir_sample_variance(get_ir_sample_means(df_1))
cis_1 <- get_ir_cis_full(z_bars = z_bars_1,
                       sample_vars = sample_vars_1,
                       r_runs = r_runs,
                       alpha = alpha)

df_2 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_2,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_2 <- get_ir_sample_means(df_2)
z_bars_2 <- get_ir_grand_sample_mean(get_ir_sample_means(df_2))
sample_vars_2 <- get_ir_sample_variance(get_ir_sample_means(df_2))
cis_2 <- get_ir_cis_full(z_bars = z_bars_2,
                       sample_vars = sample_vars_2,
                       r_runs = r_runs,
                       alpha = alpha)

df_3 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_3,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_3 <- get_ir_sample_means(df_3)
z_bars_3 <- get_ir_grand_sample_mean(get_ir_sample_means(df_3))
sample_vars_3 <- get_ir_sample_variance(get_ir_sample_means(df_3))
cis_3 <- get_ir_cis_full(z_bars = z_bars_3,
                       sample_vars = sample_vars_3,
                       r_runs = r_runs,
                       alpha = alpha)

df_4 <- run_ir_games(r_runs = r_runs,
                   m_obs = m_obs,
                   S_17 = TRUE,
                   logic_board = lb_4,
                   seed = 3489, 
                   required_cards = 52)
  
s_means_4 <- get_ir_sample_means(df_4)
z_bars_4 <- get_ir_grand_sample_mean(get_ir_sample_means(df_4))
sample_vars_4 <- get_ir_sample_variance(get_ir_sample_means(df_4))
cis_4 <- get_ir_cis_full(z_bars = z_bars_4,
                       sample_vars = sample_vars_4,
                       r_runs = r_runs,
                       alpha = alpha)

kable(list(cis_3, 
           cis_4 %>% 
             remove_rownames(),
           cis_1,
           cis_2 %>% 
             remove_rownames()),
      caption = 'Strategy Means and CIs: Simplified, Mimic Dealer, Baldwin, Thorp',
      booktabs = TRUE,
      valign = 't',
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


The first general result to be noted is the accuracy when compared to Baldwin *et al.*'s results from 1956. The simulated mean of `r round(cis_1[2, 3], 6)` comes in slightly below their own -0.0062, but safely within the `r paste0(((1 - alpha) * 100), "%")` confidence interval of `r paste0("(", paste0(round(cis_1[c(1, 3), 3], 6), collapse = ", "), ")")`. Their calculated result for mimicking the dealer of -0.056 was also spot on, while Thorp's -0.0573 seems slightly too pessimistic but also falls squarely within the `r paste0(((1 - alpha) * 100), "%")` confidence interval of `r paste0("(", paste0(round(cis_4[c(1, 3), 3], 6), collapse = ", "), ")")`. The bet value for mimicking the dealer is also always 1, which is the expected value when a player never splits or doubles on an initial bet of 1, making the net results identical to the game outcomes. 

Thorp's improvements do seem to lead to a slightly better net result, but the simulation results don't match Thorp's own numbers quite as well. Taking the rather large range of +0.01 to -0.01 for Thorp's expected value of 0.0013 for granted, this would still place the lower end of this value (-0.0087) within the simulated confidence interval of `r paste0("(", paste0(round(cis_2[c(1, 3), 3], 6), collapse = ", "), ")")`, but it does feel unsatisfactory, particularly when Thorp hints at a possible positive expectation. However, the lack of more detailed information on Thorp's numbers makes more rigorous comparison between his results and the simulated results very difficult if not impossible. In Thorp's book, percentage expectations are thrown about with wild abandon for a great variety of blackjack strategies, for rule variations, and even for completely different games, all with nary a variance or confidence interval in sight. Statistical examples given are generally small, vary greatly, and lack detailed information.^[For example Thorp, *Beat the Dealer*, 31-34, 46-48, including an expected value for US roulette ranging from -0.027 to -0.0526. Baldwin *et al.* do provide a variance of 1.1 for their expectation of -0.0062, but give no further details on how this was computed.] 

However, there are some small differences in assumptions and method between Baldwin *et al.*, Thorp, and our own simulations above, not all of which could be brought into congruence and which could lead to different results. For one, Thorp's basic strategy comes with a number of exceptions where it is unclear whether they are part of the basic strategy or not. These depend on specific combinations of cards within the matrices, for example standing with two 7's against a dealer 10, listed as a "refinement" of the basic strategy.^[There are 5 such refinements, marked with stars and crosses in Thorp's tables, see Thorp, *Beat the Dealer*, 29-31.] For simplicity these have been left out here, and their effect on the net results is thus unclear. Another difference has been mentioned earlier in relation to Baldwin *et al.*, which is the assumption of independence between hands during a game. This is generally mathematically convenient but unrealistic, and the simulated games here draw from a common deck instead, without replacement until the next game.^[For example, Thorp, *Beat the Dealer*, 37, on dealer probabilities and dealer mimicking strategies, where Thorp assumes "stochastic independence, not strictly valid but good in this instance to a high degree of approximation when the deck is nearly complete".] How these different assumptions and exceptions affect the outcome could be explored further. 

What about the claim that the simplified strategy "will only cost you about one hand in 12 hours of play"? Assuming that this difference of one hand in 12 hours is equal to the half length of a confidence interval, that 1 hand in 12 hours at 30 per hour (rather slow, worst case) translates to 1 lost hand out of 360 hands, resulting in an expected value difference of around -0.002777, and that the simulation results from Thorp's basic strategy equals "absolute perfect blackjack play", we can check whether the difference between the means of the simulated results from the simplified strategy and Thorp's strategy allows for this difference of -0.00277 or not.

```{r}
simp_vs_thorp <- get_mean_diff_ci(x_bar = z_bars_3$net_mean,
                                  y_bar = z_bars_2$net_mean,
                                  alpha = 0.05,
                                  s_var_x = sample_vars_3$net_variance,
                                  s_var_y = sample_vars_2$net_variance,
                                  n = r_runs,
                                  m = r_runs)
```


The code currently does not support common random numbers and we will have to do with an approximate confidence interval for a difference in means.^[As each simulation uses the same starter seed for its replications, it begins a run using common random numbers, but only until a difference in strategy leads to a different number of cards being drawn, after which they diverge until the next run is started. Because there is so little difference between Baldwin *et al.* and Thorp, this does still result in identical outcomes for `r round(sum(df_1[, 1] == df_2[, 1]) / (r_runs * m_obs), 4) * 100`% of all simulated games between Baldwin *et al.* and Thorp, but it is not rigorous enough to assume CRNs.] Given an alpha of 0.05, the confidence interval for the difference between the two means is `r paste0("(", paste(round(simp_vs_thorp[c(1, 3)], 6), collapse = ", "), ")")`, with a mean difference of `r round(simp_vs_thorp[c(2)], 6)`. First, seeing as how this is entirely negative, Thorp's strategy has to be assumed to be better than the simplified version. 

Second, while -0.002777 falls within this confidence interval, the lower bound is actually lower than -0.002777. This means that the claim that this simplified strategy will cost at most 1 hand in 12 hours against "perfect" blackjack will have to be rejected. Assuming 360 hands in 12 hours, the worst additional loss compared to Thorp at a 95% confidence interval is `r abs(round(simp_vs_thorp[c(1)] * 360, 6))` hands or `r round(abs(round(simp_vs_thorp[c(1)] * 360, 6)))` when rounded. Still, if you think it is acceptable to play with an expectation lower by `r round(simp_vs_thorp[c(3)], 6)` (with a confidence level of 95%) compared to Thorp's strategy - or in blackjack parlance, lose at most `r round(abs(round(simp_vs_thorp[c(1)] * 360, 6)))` more hand in 12 hours of play - then a simplified strategy such as the one included here might be worth using.

```{r}
baldwin_vs_thorp <- get_mean_diff_ci(x_bar = z_bars_1$net_mean,
                                  y_bar = z_bars_2$net_mean,
                                  alpha = 0.05,
                                  s_var_x = sample_vars_1$net_variance,
                                  s_var_y = sample_vars_2$net_variance,
                                  n = r_runs,
                                  m = r_runs)
```

Finally, which of these strategies is the most profitable? Given that mimicking the dealer is so much worse than the other three strategies, this strategy won't be taken into further consideration. Above, the simplified strategy has also already proven to be statistically worse than Thorp's, which leaves us with Baldwin *et al.* against Thorp. From their net result confidence intervals alone they look nearly indistinguishable, and graphically as in chart 2 below they are also almost identical. Given an alpha of 0.05, the confidence interval for the difference between their two means is `r paste0("(", paste(round(baldwin_vs_thorp[c(1, 3)], 6), collapse = ", "), ")")`, with a mean difference of `r round(baldwin_vs_thorp[c(2)], 6)`. With 0 being within this confidence interval, we can't reject that they are equal and the result is thus inconclusive. Interestingly, the worst expected difference at 360 hands per 12 hours would even here be `r abs(round(baldwin_vs_thorp[c(1)] * 360, 2))` additional lost hands. Clearly, remarks on hands lost per duration of play shouldn't be made lightly.






\vspace{12pt}

```{r}
strat_sample_means <- get_ir_sample_means(df_1) %>% 
  mutate(Strategy = "Baldwin") %>%
  bind_rows(
    get_ir_sample_means(df_2) %>% 
      mutate(Strategy = "Thorp"),
    get_ir_sample_means(df_3) %>% 
      mutate(Strategy = "Simplified"),
    get_ir_sample_means(df_4) %>% 
      mutate(Strategy = "Mimic Dealer"))

ggplot(strat_sample_means, aes(x=net_means, color=Strategy)) +
  geom_histogram(alpha=0.2, position="identity", binwidth = 0.005) +
  scale_color_manual(values=c("#FFFF00", "#CC3300", "#00CC00", "#0000FF")) +
  scale_x_continuous(breaks = round(seq(min(strat_sample_means$net_means), 
                                        max(strat_sample_means$net_means), 
                                        by = 0.005), 2)) +
  theme_minimal() +
  xlab("Percentages") +
  ggtitle("Chart 2: Histograms of Strategy Sample Means of Net Results")
```


# Conclusions


\newgeometry{left=0.2in,right=0.2in,top=0.2in,bottom=0.2in}

# Appendix

## Strategy Tables


```{r}

kable(list(lb_h1[6:14, ], lb_s1[-1,], lb_sp1), 
      caption = "Baldwin et al.'s Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")

```



```{r}
kable(list(lb_h2[6:14, ], lb_s2[-1,], lb_sp2), 
      caption = "Thorp's Basic Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```

```{r}
kable(list(lb_h3[6:14, ], lb_s3[-1,], lb_sp3), 
      caption = "Simplified Basic Strategy (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


```{r}
kable(list(lb_h4[6:14, ], lb_s4[-1,], lb_sp4), 
      caption = "Mimic Dealer (hard, soft, split)",
      booktabs = TRUE,
      valign = 't', 
      linesep = "") %>% 
  kable_paper() %>% 
  kable_styling(latex_options = "HOLD_position")
```


\restoregeometry







